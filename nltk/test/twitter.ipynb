{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Steps\n",
    "\n",
    "In order to collect data from Twitter, you first need to register a new *application* &mdash; this is Twitter's way of referring to any computer program that interacts with the Twitter API. As long as you save your registration information correctly, you should only need to do this once, since the information should work for any NLTK code that you write. You will need to have a Twitter account before you can register. Twitter also insists that [you add a mobile phone number to your Twitter profile](https://support.twitter.com/articles/110250-adding-your-mobile-number-to-your-account-via-web) before you will be allowed to register an application.\n",
    "\n",
    "These are the steps you need to carry out.\n",
    "\n",
    "### Getting your API keys from Twitter\n",
    "\n",
    "1. Sign in to your Twitter account at https://apps.twitter.com. You should then get sent to a screen that looks something like this:\n",
    "<img src=\"images/twitter_app1.tiff\" width=\"600px\">\n",
    "Clicking on the **Create New App** button should take you to the following screen:\n",
    "<img src=\"images/twitter_app2.tiff\" width=\"600px\">\n",
    "The information that you provide for **Name**, **Description** and **Website** can be anything you like.\n",
    "\n",
    "2. Make sure that you select **Read and Write** access for your application (as specified on the *Permissions* tab of Twitter's Application Management screen):\n",
    "<img src=\"images/twitter_app3.tiff\" width=\"600px\">\n",
    "\n",
    "3. Go to the tab labeled **Keys and Access Tokens**. It should look something like this, but with actual keys rather than a string of Xs:\n",
    "<img src=\"images/twitter_app4.png\" width=\"650px\">\n",
    "As you can see, this will give you four distinct keys: consumer key, consumer key secret, access token and access token secret.\n",
    "\n",
    "### Storing your keys\n",
    "1. Create a folder named `twitter-files` in your home directory. Within this folder, use a text editor to create a new file called `credentials.txt`. Make sure that this file is just a plain text file. In it, you should create which you should store in a text file with the following structure:\n",
    "```\n",
    "app_key=YOUR CONSUMER KEY  \n",
    "app_secret=YOUR CONSUMER SECRET  \n",
    "oauth_token=YOUR ACCESS TOKEN  \n",
    "oauth_token_secret=YOUR ACCESS TOKEN SECRET\n",
    "```\n",
    "Type the part up to and includinge the '=' symbol exactly as shown. The values on the right-hand side of the '=' &mdash; that is, everything in caps &mdash; should be cut-and-pasted from the relevant API key information shown on the Twitter **Keys and Access Tokens**. Save the file and that's it.\n",
    "\n",
    "2. It's going to be important for NLTK programs to know where you have stored your credentials. We'll assume that this folder is called `twitter-files`, but you can call it anything you like. We will also assume that this folder is where you save any files containing tweets that you collect. Once you have decided on the name and location of this folder, you will need to set the TWITTER environment variable to this value. For example, on a Unix-like system, you can use something like this:\n",
    "```bash\n",
    "export TWITTER=\"/path/to/your/twitter-files\"\n",
    "```\n",
    "On a Windows machine, right click on ‚ÄúMy Computer‚Äù then select `Properties > Advanced > Environment Variables > User Variables > New...` One important thing to remember is that you need to keep your `credentials.txt` file private. So do **not** share your `twitter-files` folder with anyone else, and do not upload it to a public repository such as GitHub.\n",
    "\n",
    "3. Finally, read through Twitter's [Developer Rules of the Road](https://dev.twitter.com/overview/terms/policy). As far as these rules are concerned, you count as both the application developer and the User.\n",
    "\n",
    "### Install Twython\n",
    "The NLTK Twitter package relies on a third part library called [Twython](https://twython.readthedocs.org/). Install Twython via [pip](https://pip.pypa.io):\n",
    "```bash\n",
    "$ pip install twython\n",
    "```\n",
    "\n",
    "or with [easy_install](https://pythonhosted.org/setuptools/easy_install.html):\n",
    "\n",
    "```bash\n",
    "$ easy_install twython\n",
    "```\n",
    "We're now ready to get started. The next section will describe how to use the `Twitter` class to talk to the Twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*More detail*:\n",
    "Twitter offers are two main authentication options. OAuth 1 is for user-authenticated API calls, and allows sending status updates, direct messages, etc, whereas OAuth 2 is for application-authenticated calls, where read-only access is sufficient. Although OAuth 2 sounds more appropriate for the kind of tasks envisaged within NLTK, it turns out that access to Twitter's streaming API requires OAuth 1, which is why it's necessary to obtain *Read and Write* access for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the simple `Twitter` class\n",
    "\n",
    "The `Twitter` class is intended as a simple means of interacting with the Twitter data stream. Later on, we'll look at other methods which give more fine-grained control.  \n",
    "\n",
    "The first example looks for tweets from the live public stream which include either  the word *love* or *hate*. We limit the call to finding 10 tweets. When you run this code, it will definitely produce different results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tellin everyone I hate you..... I don't hate you I just don't like you üíÅ\n",
      "hi Luke Hemmings from 5sos\n",
      "my birthday is in 35 days\n",
      "and your follow would be the best present ever\n",
      "i love you so much\n",
      "@Luke5SOS\n",
      "@5SOS\n",
      "x511\n",
      "Àö‡Æê‚Çä‚úß(‚ùù·∑Ä‡©å‚ÄàÀôÃÆ‚Äà‚ùù·∑Ä‡©å‡•¢‚óè)‚Å∫À≥‡Æê‡ºö\n",
      "luke hemmings, hey sunshine!\n",
      "it would mean the world if \n",
      "you could follow me.\n",
      "i love you forever\n",
      "@luke5sos Âπ∏„Åõ\n",
      "9997\n",
      "60 million. Love all of u. #Beliebers\n",
      "If you love me you'll get me coffee.\n",
      "RT @LAY9107: love you EXO LAY ‚ô°‚ô°‚ô° #3yearswithEXO http://t.co/HDW8jmOATf\n",
      "RT @BsbLifestyle__: *hits double*\n",
      "\n",
      "\"Damn, I love baseball!\"\n",
      "\n",
      "*strikes out looking*\n",
      "\n",
      "\"I FUCKING HATE BASEBALL!  FUCK!\"\n",
      "RT @qtjyh: Junsu: *sings somebody to love*\n",
      "\n",
      "Changmin: But hyung it's more epic if you say shimbody to love.\n",
      "\n",
      "Junsu: get out\n",
      "RT @toffeegee: SUKIRA - Kai \"Just like having a one and only girlfriend. I love you all\" http://t.co/gtOSmNCwtx\n",
      "@Louis_Tomlinson I cant even begin to describe how happy u and the guys make me. Im so proud of u. I fucking love u. Follow me please 75.505\n"
     ]
    }
   ],
   "source": [
    "from nltk.twitter import Twitter\n",
    "tw = Twitter()\n",
    "tw.tweets(keywords='love, hate', limit=10) #sample from the public stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example filters the live public stream by looking for specific accounts. In this case, we 'follow' two news organisations, namely `@CNN` and `@BBCNews`. Following [Twitter's advice](https://dev.twitter.com/streaming/reference/post/statuses/filter), we use *numeric userIDs* for these accounts. Although we could write some Python code to convert usernames such as `@CNN` to userIDs such as `759251`, for now you might find it simpler to use a web service like [TweeterID](http://tweeterid.com) if you want to try out following different accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @BBCNews: Woman charged with murdering son, 5, is acquitted; Scottish prosecutors accept she was insane http://t.co/diTM2k6TZf\n",
      "RT @BBCNews: Thieves target vault &amp; safety-deposit boxes in London's #HattonGarden jewellery district http://t.co/h17qnzGubx\n",
      "RT @BBCPolitics: David Cameron tells Welsh voters \"the plan's working - stick with the plan\" http://t.co/wApA5D64CD #GE2015 http://t.co/3sr‚Ä¶\n",
      "RT @CNN: #MarchMadness #CNNBrackets winner is @JohnKingCNN, followed by @BillWeirCNN &amp; @SanjayGuptaCNN http://t.co/AajK0D96Qx http://t.co/S‚Ä¶\n",
      "RT @BBCR1: Greg dropped the @BBCNews theme tune while DJing this weekend and it went OFF!\n",
      "https://t.co/01mZg6WUCI\n",
      "RT @CNN: A North Korean defector has sent thousands of copies of @TheInterview to N. Korea in balloons http://t.co/dNjKSluKJQ\n",
      "https://t.co/‚Ä¶\n",
      "RT @CNN: Starbucks to give workers a full ride for college. http://t.co/vpQXbX8p08\n",
      "RT @BBCR1: Greg dropped the @BBCNews theme tune while DJing this weekend and it went OFF!\n",
      "https://t.co/01mZg6WUCI\n",
      "RT @CNN: A North Korean defector has sent thousands of copies of @TheInterview to N. Korea in balloons http://t.co/dNjKSluKJQ\n",
      "https://t.co/‚Ä¶\n",
      "RT @BBCR1: Greg dropped the @BBCNews theme tune while DJing this weekend and it went OFF!\n",
      "https://t.co/01mZg6WUCI\n"
     ]
    }
   ],
   "source": [
    "tw = Twitter()\n",
    "tw.tweets(follow=['759251', '612473'], limit=10) # see what CNN and BBC are talking about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's fun to view the Twitter stream going by on your screen, you'll probably want to save some tweets in a file. We can tell the `tweets()` method to save to a file by setting the flag `to_screen` to `False`. The `Twitter` class will look at the value of your environmental variable `TWITTER` to determine which folder to use to save the tweets, and it will put them in a date-stamped file with the prefix `tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/ewan/twitter/tweets.20150407-165111.json\n",
      "Written 25 tweets\n"
     ]
    }
   ],
   "source": [
    "tw = Twitter()\n",
    "tw.tweets(to_screen=False, limit=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've been taking data from the live public stream. However, it's also possible to retrieve past tweets, for example by searching for specific keywords, and setting `stream-False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new favorite: Voyeur - Philae &amp; Rosetta [Madhouse Records] by @madhouserecords https://t.co/Vfh45AxECn on #SoundCloud\n",
      "@theeconomist\n",
      "Yes but some projects are awesome still !Like Philae and Rosetta mission who are successful :)\n",
      "Celebrating ESA's Rosetta Mission with promo access to a planetary science Special Virtual Issue - 32 papers!  http://t.co/q6ocPKkFIz\n",
      "RT @simonrae: #Philly ... having a relaxed #Easter?\n",
      "\n",
      "@Philae2014 @Philae_MUPUS @ESA_Rosetta @Philae_Ptolemy http://t.co/nkhiRNWGFw\n",
      "#Reading : OSIRIS spots Philae drifting across the comet http://t.co/ra2VpeUWpz @florencemeichel\n",
      "DLR Portal - Rosetta - Geduld mit Lander Philae http://t.co/9YT4cOtSQv\n",
      "RT @simonrae: #Philly ... having a relaxed #Easter?\n",
      "\n",
      "@Philae2014 @Philae_MUPUS @ESA_Rosetta @Philae_Ptolemy http://t.co/nkhiRNWGFw\n",
      "Bill Nye on Rosetta Comet Landing: We'll make discoveries that nobody's imagined yet. http://t.co/JRFJ2f1Bnh via @bigthink\n",
      "#Google The #remarkable #mosaic of images revealed by Esa in a #blogpost was http://t.co/AP7Fyc3dvo\n",
      "RT @simonrae: #Philly ... having a relaxed #Easter?\n",
      "\n",
      "@Philae2014 @Philae_MUPUS @ESA_Rosetta @Philae_Ptolemy http://t.co/nkhiRNWGFw\n"
     ]
    }
   ],
   "source": [
    "tw.tweets(keywords='philae rosetta', stream=False, limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Tweet Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Extracting Parts of a Tweet\n",
    "Now let's see how to process a file that contains a series of Tweets, for example those that we might collect with the function `tweets()`. \n",
    "\n",
    "For demonstration purposes, we will use one of the files of Tweets that is distributed as part of the NLTK corpus, namely `tweets.20150430-223406.json`. The `abspath()` method of the corpus gives us the full pathname of the relevant file. If your NLTK data is installed in the default location on a Unix-like system, this pathname will be `'/usr/share/nltk_data/corpora/twitter_samples/tweets.20150430-223406.json'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "input_file = twitter_samples.abspath(\"tweets.20150430-223406.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function `json2csv()` takes as input a file-like object consisting of Tweets as line-delimited [JSON](http://www.json.org) objects and returns a file in CSV format. The third parameter of the function lists the fields that we want to extract from the JSON. One of the most useful, and simplest, examples is to extract just the text of the Tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.twitter.util import json2csv\n",
    "with open(input_file) as fp:\n",
    "    json2csv(fp, 'tweets_text.csv', ['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've passed the filename `'tweets_text.csv'` as the second argument of `json2csv()`. Unless you provide a complete pathname, the file will be created in the directory where you are currently executing Python.\n",
    "\n",
    "If you open the file `'tweets_text.csv'`, the first 5 lines should look as follows:\n",
    "\n",
    "```\n",
    "RT @KirkKus: Indirect cost of the UK being in the EU is estimated to be costing Britain ¬£170 billion per year! #BetterOffOut #UKIP\n",
    "VIDEO: Sturgeon on post-election deals http://t.co/BTJwrpbmOY\n",
    "RT @LabourEoin: The economy was growing 3 times faster on the day David Cameron became Prime Minister than it is today.. #BBCqt http://t.co‚Ä¶\n",
    "RT @GregLauder: the UKIP east lothian candidate looks about 16 and still has an msn addy http://t.co/7eIU0c5Fm1\n",
    "RT @thesundaypeople: UKIP's housing spokesman rakes in ¬£800k in housing benefit from migrants.  http://t.co/GVwb9Rcb4w http://t.co/c1AZxcLh‚Ä¶\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in most of the analysis of data coming from Twitter, some of the tweet metadata will be very useful, e.g.: the creation date, the user. The object Tweet and all its attributes are described in this [link](https://dev.twitter.com/overview/api/tweets). From the same output file generated by `tweets()`, it is possible to accesss them by modifying the list in the 3rd argument.\n",
    "\n",
    "For example, the following will generate a csv including most of the metadata of the tweet and the id of the user who has generated it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(input_file) as fp:\n",
    "    json2csv(fp, 'tweets.20150430-223406.tweet.csv',\n",
    "            ['created_at', 'favorite_count', 'id', 'in_reply_to_status_id', \n",
    "            'in_reply_to_user_id', 'retweet_count', 'retweeted', \n",
    "            'text', 'truncated', 'user.id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the first 9 elements of the list are attributes of the tweet, while the last one, `user.id`, takes the user object, also for the tweet, and gets the attributes in the list (in this case only the id). The object for the Twitter User is described in this [link](https://dev.twitter.com/overview/api/users)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the metadata of the tweet are the so called [entities](https://dev.twitter.com/overview/api/entities) and [places](https://dev.twitter.com/overview/api/places). The following are examples on how to get each of those entities. They all include the id of the tweet as the first argument, and some of them include also the text for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.twitter.util import json2csv_entities\n",
    "with open(input_file) as fp:\n",
    "    json2csv_entities(fp, 'tweets.20150430-223406.hashtags.csv',\n",
    "                        ['id', 'text'], 'hashtags', ['text'])\n",
    "    \n",
    "with open(input_file) as fp:\n",
    "    json2csv_entities(fp, 'tweets.20150430-223406.user_mentions.csv',\n",
    "                        ['id', 'text'], 'user_mentions', ['id', 'screen_name'])\n",
    "    \n",
    "with open(input_file) as fp:\n",
    "    json2csv_entities(fp, 'tweets.20150430-223406.media.csv',\n",
    "                        ['id'], 'media', ['media_url', 'url'])\n",
    "    \n",
    "with open(input_file) as fp:\n",
    "    json2csv_entities(fp, 'tweets.20150430-223406.urls.csv',\n",
    "                        ['id'], 'urls', ['url', 'expanded_url'])\n",
    "    \n",
    "with open(input_file) as fp:\n",
    "    json2csv_entities(fp, 'tweets.20150430-223406.place.csv',\n",
    "                        ['id', 'text'], 'place', ['name', 'country'])\n",
    "\n",
    "with open(input_file) as fp:\n",
    "    json2csv_entities(fp, 'tweets.20150430-223406.place_bounding_box.csv',\n",
    "                        ['id', 'name'], 'place.bounding_box', ['coordinates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, when a tweet is actually a retweet, the original tweet can be also fetched from the same file, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(input_file) as fp:\n",
    "    json2csv_entities(fp, 'tweets.20150430-223406.original_tweets.csv',\n",
    "                        ['id'], 'retweeted_status', ['created_at', 'favorite_count', \n",
    "                        'id', 'in_reply_to_status_id', 'in_reply_to_user_id', 'retweet_count',\n",
    "                        'text', 'truncated', 'user.id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the 1st id corresponds to the retweeted tweet, and the 2nd id to the original tweet.\n",
    "\n",
    "Additionally, all those csv files can be easily read as `pandas` dataframes. [Pandas](http://pandas.pydata.org/) is a data analysis library for python, very handy when manipulating tabular data. As an example, for the 2nd csv including some of the tweet data and the id of the user, it would be as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('tweets.20150430-223406.tweet.csv', index_col=2, header=0, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that dataframe it is easy, for example, to obtain the text of the tweets for a user id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "593891099548094465    VIDEO: Sturgeon on post-election deals http://...\n",
       "593891101766918144    SNP leader faces audience questions http://t.c...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.loc[tweets['user.id'] == 557422508]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
